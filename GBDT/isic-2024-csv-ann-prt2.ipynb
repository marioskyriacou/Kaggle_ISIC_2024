{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport time \nimport matplotlib\nimport tensorflow as tf\nimport random\nimport seaborn as sns \nimport imblearn\n\nprint(f'Numpy: {np.__version__}')\nprint(f'Pandas: {pd.__version__}')\nprint(f'Sklearn: {sklearn.__version__}')\nprint(f'Matplotlib: {matplotlib.__version__}')\nprint(f'TensorFlow:{tf.__version__}')\nprint(f'Imb-Learn:{imblearn.__version__}')\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-05T12:55:10.208906Z","iopub.execute_input":"2024-09-05T12:55:10.210130Z","iopub.status.idle":"2024-09-05T12:55:31.756185Z","shell.execute_reply.started":"2024-09-05T12:55:10.210068Z","shell.execute_reply":"2024-09-05T12:55:31.754386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n\n# DNN \nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.losses import BinaryCrossentropy, categorical_crossentropy\n\n# Validation metrics\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_curve, auc\n\n# Sampling \nfrom imblearn.over_sampling import SMOTE","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.758875Z","iopub.execute_input":"2024-09-05T12:55:31.759765Z","iopub.status.idle":"2024-09-05T12:55:31.872255Z","shell.execute_reply.started":"2024-09-05T12:55:31.759714Z","shell.execute_reply":"2024-09-05T12:55:31.870720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Varibales ","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\ntest_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.874111Z","iopub.execute_input":"2024-09-05T12:55:31.874534Z","iopub.status.idle":"2024-09-05T12:55:31.880781Z","shell.execute_reply.started":"2024-09-05T12:55:31.874492Z","shell.execute_reply":"2024-09-05T12:55:31.878894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# common columns\n#train_cols_set, test_cols_set = set(train_data.columns), set(test_data.columns)\n#cols = list(test_cols_set.intersection(train_cols_set))\n\ncommon_cols = ['sex', 'tbp_lv_deltaLB', 'tbp_lv_location_simple', 'tbp_lv_areaMM2', 'tbp_lv_deltaL', 'tbp_lv_norm_border', 'tbp_lv_A',\n 'tbp_lv_B', 'tbp_lv_symm_2axis', 'tbp_lv_L', 'tbp_lv_location', 'tbp_lv_deltaLBnorm', 'tbp_lv_radial_color_std_max',\n 'tbp_lv_Lext',  'tbp_lv_Bext', 'tbp_lv_H', 'tbp_lv_perimeterMM', 'tbp_lv_stdL', 'tbp_lv_minorAxisMM', 'tbp_lv_stdLExt',\n 'tbp_lv_y', 'tbp_lv_color_std_mean', 'clin_size_long_diam_mm', 'age_approx', 'tbp_lv_eccentricity', 'tbp_lv_deltaA',\n 'tbp_tile_type', 'tbp_lv_Hext', 'tbp_lv_C', 'tbp_lv_norm_color', 'tbp_lv_deltaB', 'anatom_site_general', 'tbp_lv_x',\n 'tbp_lv_Aext',  'tbp_lv_Cext', 'tbp_lv_symm_2axis_angle', 'tbp_lv_nevi_confidence', 'tbp_lv_z', 'tbp_lv_area_perim_ratio']","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.884190Z","iopub.execute_input":"2024-09-05T12:55:31.884746Z","iopub.status.idle":"2024-09-05T12:55:31.898190Z","shell.execute_reply.started":"2024-09-05T12:55:31.884701Z","shell.execute_reply":"2024-09-05T12:55:31.896366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# patient_id: Unique patient identifier. (object)\n# image_type: Structured field of the ISIC Archive for image type. (o)\n# copyright_license: Copyright license. (object)\nto_drop = ['isic_id','patient_id','image_type', 'copyright_license', 'attribution']","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.900177Z","iopub.execute_input":"2024-09-05T12:55:31.900737Z","iopub.status.idle":"2024-09-05T12:55:31.922302Z","shell.execute_reply.started":"2024-09-05T12:55:31.900681Z","shell.execute_reply":"2024-09-05T12:55:31.920909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical columns \ncat_cols_list = ['sex',\n                 'tbp_lv_location_simple', \n                 'tbp_lv_location', \n                 'tbp_tile_type', \n                 'anatom_site_general']\n\n# mapping \nsex = {'male':0, 'female':1}\n\ntbp_lv_location_simple =  {'Head & Neck': 0, \n                           'Left Arm': 1, \n                           'Left Leg': 2, \n                           'Right Arm': 3, \n                           'Right Leg': 4, \n                           'Torso Back': 5, \n                           'Torso Front': 6}\n\ntbp_lv_location = {'Head & Neck': 0, 'Left Arm': 1,'Left Arm - Lower': 2, 'Left Arm - Upper': 3,'Left Leg': 4, \n                   'Left Leg - Lower': 5, 'Left Leg - Upper': 6, 'Right Arm': 7, 'Right Arm - Lower': 8, 'Right Arm - Upper': 9, 'Right Leg': 10, 'Right Leg - Lower': 11,\n  'Right Leg - Upper': 12, 'Torso Back': 13, 'Torso Back Bottom Third': 14, 'Torso Back Middle Third': 15, 'Torso Back Top Third': 16,\n  'Torso Front': 17, 'Torso Front Bottom Half': 18, 'Torso Front Top Half': 19}\n\ntbp_tile_type = {'3D: XP': 0, '3D: white': 1}\n\nanatom_site_general = {'anterior torso': 0, 'head/neck': 1, 'lower extremity': 2, 'posterior torso': 3, 'upper extremity': 4}\n\n# genral dictionary\ncat_cols = {'sex': sex, \n           'tbp_lv_location_simple':tbp_lv_location_simple, \n           'tbp_lv_location': tbp_lv_location, \n           'tbp_tile_type':tbp_tile_type,\n           'anatom_site_general': anatom_site_general}","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.924645Z","iopub.execute_input":"2024-09-05T12:55:31.925132Z","iopub.status.idle":"2024-09-05T12:55:31.938952Z","shell.execute_reply.started":"2024-09-05T12:55:31.925070Z","shell.execute_reply":"2024-09-05T12:55:31.937234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Feature Extraction Section uses Chi-Square, ANOVA F-score, Fisher score and mRMR method to identify the most relevat features. Then Intersection is used to find the common features.  \n\nChi 2: 27 \\\nANOVA:32 \\\nFisher:32 \\\nmRMR:22 \\\nCommon features:15","metadata":{}},{"cell_type":"code","source":"intersection_cols = ['sex', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_areaMM2',\n  'tbp_lv_color_std_mean', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_location', 'tbp_lv_location_simple',\n  'tbp_lv_minorAxisMM', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max',\n  'tbp_lv_stdL']\n\nintersection_cols.append('target')\nprint(f'16 selected columns +1 target  = {len(intersection_cols)}')","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.940736Z","iopub.execute_input":"2024-09-05T12:55:31.941146Z","iopub.status.idle":"2024-09-05T12:55:31.958360Z","shell.execute_reply.started":"2024-09-05T12:55:31.941106Z","shell.execute_reply":"2024-09-05T12:55:31.956330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classes","metadata":{}},{"cell_type":"code","source":"class Data:\n    \n    def __init__(self, path, common_columns, categorical_cols, target_col):\n        self.path = path\n        self.common_columns = common_columns\n        self.categorical_cols = categorical_cols\n        self.target_col = target_col\n    \n    def read_data(self):\n        #read data\n        self.df = pd.read_csv(self.path,  low_memory=False)\n        print(f'Data Shape:{self.df.shape}')\n\n    def data_prep(self):\n        \n        if self.target_col: \n            self.common_columns.append('target')      \n            \n        self.df = self.df[self.common_columns] # df with common cols+target \n        print(f'Common Cols:{len(self.common_columns)}.Data Shape: {self.df.shape}')  \n        \n        # drop missing values\n        missing_vals = self.df.isnull().sum().sum()\n        if missing_vals > 0:\n            self.df = self.df.dropna()\n            self.df = self.df.reset_index (drop = True)\n            \n        print(f'Missing Vals:{missing_vals}.Data Shape: {self.df.shape}')\n        \n        # remove dublicate \n        print(f'Dublicate Values:{self.df.duplicated().sum()}') # no dublicated values \n        \n        #columns mapping\n        for key in self.categorical_cols.keys():\n            self.df[key] = self.df[key].map(self.categorical_cols[key])\n        \n        # Scaling columns \n        scaling_lst = list(self.df.select_dtypes(exclude=['int64']).dtypes.index)\n        # categorical columns with more than 2 unique values\n        scaling_lst.append('tbp_lv_symm_2axis_angle')\n        scaling_lst.append('tbp_lv_location_simple')\n        scaling_lst.append('tbp_lv_location')\n        scaling_lst.append('anatom_site_general')\n\n        scaler = StandardScaler()\n        self.df[scaling_lst] = scaler.fit_transform(self.df[scaling_lst])\n        \n        # shuflle data\n        self.df = self.df.sample(frac=1).reset_index(drop=True)\n        print(f'Data Shape: {self.df.shape}')\n        \n        if self.target_col:\n            self.common_columns.remove('target')\n            \n        print(f'Final Shape: {self.df.shape}')\n        \n    def get_data(self):\n        return self.df\n        ","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.960555Z","iopub.execute_input":"2024-09-05T12:55:31.961139Z","iopub.status.idle":"2024-09-05T12:55:31.980654Z","shell.execute_reply.started":"2024-09-05T12:55:31.961082Z","shell.execute_reply":"2024-09-05T12:55:31.979073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataFrame_Split:\n    \"\"\"\n    Aim: create two sub-datasets from the main data file. \n    Each sub-dataset should:\n        Contain the same number of attack instances.\n        Have an equal of normal samples.\n    Return: List of Dfs \n    \"\"\"\n    def __init__(self, df):\n        self.df = df  \n        \n        # normal // mal index \n        norm_indx = self.df[self.df['target'] == 0].index.to_list()\n        mal_indx = self.df[self.df['target'] == 1].index.to_list()\n        \n        # normal // mal df\n        self.norm_df = self.df.iloc[norm_indx]\n        self.mal_df = self.df.iloc[mal_indx]\n        print(f'Malicus df: {self.mal_df.shape} .... Normal df: {self.norm_df.shape}')\n        \n    def create_random_subsets(self, data, n_splits):\n        shuffled_data = data.sample(frac=1).reset_index(drop=True) # Shuffle the dataset\n        avg_size = len(shuffled_data) // n_splits # size of each subset\n\n        subsets = []\n        for i in range(n_splits):\n            start_idx = i * avg_size\n            if i == n_splits - 1:  # Make sure the last split gets all remaining data\n                subsets.append(shuffled_data[start_idx:])\n            else:\n                subsets.append(shuffled_data[start_idx:start_idx + avg_size])\n        \n        return  subsets\n        \n                \n    def main(self):\n        norm_sub_dfs = self.create_random_subsets(self.norm_df, n_splits=2)\n        \n        self.sub_dfs = []\n        for i in range(len(norm_sub_dfs)): \n            \n            sub_df = pd.concat([norm_sub_dfs[i], self.mal_df], ignore_index=True) # concat dfs \n            sub_df = sub_df.sample(frac=1).reset_index(drop=True) # suffle \n            print(f'Data_{i}: {sub_df.shape}')\n            self.sub_dfs.append(sub_df)\n                \n    def get_sub_df(self):\n        return  self.sub_dfs","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:31.983010Z","iopub.execute_input":"2024-09-05T12:55:31.983579Z","iopub.status.idle":"2024-09-05T12:55:32.001808Z","shell.execute_reply.started":"2024-09-05T12:55:31.983519Z","shell.execute_reply":"2024-09-05T12:55:32.000064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Ratio_data():\n    \n    def __init__(self, df, ratio):\n        self.df = df\n        self.ratio = ratio # [0, 1]\n    \n    def data_ratio(self):\n        norm_indx = self.df[self.df.target == 0].index.to_list()\n        mal_indx = self.df[self.df.target == 1].index.to_list()\n        \n        # random select ratio of the normal data \n        norm_rand_indx = random.sample(norm_indx, int(len(norm_indx)*self.ratio) )\n        \n        # select random ratio \n        train_df_norm = self.df.iloc[norm_rand_indx]\n        train_df_mal = self.df.iloc[mal_indx]\n\n        self.sub_df = pd.concat([train_df_norm, train_df_mal])\n        self.sub_df = self.sub_df.sample(frac=1).reset_index(drop=True) # suffle\n        \n        \n    def split_data(self, test_size = 0.2):\n        features, targ = self.sub_df.columns[:-1], self.sub_df.columns[-1] # features / target\n        self.X, self.y = self.sub_df[features], self.sub_df[targ]\n        # train test split\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size, stratify=self.y)\n        print(f'Xtrain:{self.X_train.shape} // ytrain:{self.y_train.shape}\\nXval:{self.X_test.shape} // yval:{self.y_test.shape}')\n        print(f'ytrain {self.y_train.value_counts()}')\n        print(f'ytest {self.y_test.value_counts()}')\n        \n    \n    def train_smote(self):\n        sm = SMOTE(sampling_strategy = 0.5)\n        self.X_res, self.y_res = sm.fit_resample(self.X_train, self.y_train)\n        print(f'Smote Data:{self.X_res.shape}\\nTarget:{self.y_res.value_counts()}')\n        \n    def get_X_y(self):\n        return self.X, self.y\n    \n    def get_balance_data(self):\n         return self.X_res, self.y_res, self.X_test, self.y_test\n        \n    def get_imbalance_data(self):\n        return self.X_train, self.y_train, self.X_test, self.y_test","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:32.007762Z","iopub.execute_input":"2024-09-05T12:55:32.008189Z","iopub.status.idle":"2024-09-05T12:55:32.026676Z","shell.execute_reply.started":"2024-09-05T12:55:32.008150Z","shell.execute_reply":"2024-09-05T12:55:32.025015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def features_dist_plot(df):\n    \n    plt.figure(figsize=(20, 20)) \n    \n    # Loop through the first 15 features and create a subplot for each\n    for i, feature in enumerate(df.columns):\n        plt.subplot(5, 3, i + 1)  # 5 rows, 3 columns for 15 plots\n        sns.histplot(df[feature], kde=True, color='blue')\n        plt.title(f'Distribution of {feature}')\n        plt.xlabel(feature)\n        plt.ylabel('Frequency')\n\n    # Adjust layout to prevent overlap\n    plt.tight_layout()\n    plt.show()\n    \n# features_dist_plot(X_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:32.029061Z","iopub.execute_input":"2024-09-05T12:55:32.029807Z","iopub.status.idle":"2024-09-05T12:55:32.049522Z","shell.execute_reply.started":"2024-09-05T12:55:32.029730Z","shell.execute_reply":"2024-09-05T12:55:32.048017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Neural_Network:\n    \n    def __init__(self, X_train, y_train, X_val, y_val):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_val = X_val\n        self.y_val = y_val\n        \n        # Model\n        self.model = Sequential([\n            Dense(128, activation= 'relu', input_shape=(16,)),\n            tf.keras.layers.BatchNormalization(),\n    \n            Dense(128, activation= 'relu'),\n    \n            Dense(64, activation= 'relu'),\n            Dense(1, activation= 'sigmoid') # sigmoid/softmax Output layer  \n            ]) \n        \n        self.history = None  # To store training history\n        \n\n    def model_compile(self):\n        # SGD optimizer for imbalance \n        optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n        # tf.keras.optimizers.Adam(clipvalue=1.0) // adam\n        \n        self.model.compile(\n            optimizer = optimizer, \n            loss= tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, alpha=0.25, \n                                                       from_logits=False),\n            metrics=['accuracy']\n                            ) # BinaryCrossentropy\n    \n    def model_summary(self):\n        print(self.model.summary())\n        \n    def model_fit(self):\n        # Class Weights\n        # # Calculate class weights for the current fold\n        # class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n        # class_weights = {0: class_weights[0], 1: class_weights[1]}\n\n        class_weights = {0: 0.5099934469200524, 1: 25.516393442622952} # no balance data\n        self.history = self.model.fit(\n                    self.X_train.values, self.y_train.values, \n                    validation_data=(self.X_val.values, self.y_val.values), \n                    epochs=25, \n                    batch_size=128, \n                    class_weight=class_weights\n                    ) # callbacks=[early_stop], \n    def model_history(self):\n        # Loss Curves\n        plt.figure(figsize=[8,6])\n        plt.plot(self.history.history['loss'],'r',linewidth=3.0)\n        plt.plot(self.history.history['val_loss'],'b',linewidth=3.0)\n        plt.legend(['Training loss', 'Validation Loss'],fontsize=10)\n        plt.xlabel('Epochs ',fontsize=10)\n        plt.ylabel('Loss',fontsize=10)\n        plt.title('Loss Curves',fontsize=10)\n\n        # ccuracy Curves\n        plt.figure(figsize=[8,6])\n        plt.plot(self.history.history['accuracy'],'r',linewidth=3.0)\n        plt.plot(self.history.history['val_accuracy'],'b',linewidth=3.0)\n        plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=10)\n        plt.xlabel('Epochs ',fontsize=10)\n        plt.ylabel('Accuracy',fontsize=10)\n        plt.title('Accuracy Curves',fontsize=10)\n\n        plt.legend()\n        plt.show()\n\n    def model_roc_curve(self):\n        \n        y_pred_prob = self.model.predict(self.X_val).ravel()\n\n        # Step 5: Compute ROC curve and AUC\n        fpr, tpr, thresholds = roc_curve(self.y_val, y_pred_prob)\n        roc_auc = auc(fpr, tpr)\n\n        # Step 6: Plot the ROC curve\n        plt.figure()\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.0])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Receiver Operating Characteristic')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\n            \n    def get_model(self):\n        return self.model","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:32.052524Z","iopub.execute_input":"2024-09-05T12:55:32.053064Z","iopub.status.idle":"2024-09-05T12:55:32.083393Z","shell.execute_reply.started":"2024-09-05T12:55:32.053019Z","shell.execute_reply":"2024-09-05T12:55:32.081707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# train data \ndata_train = Data(path = train_path,\n                 common_columns = common_cols, \n                 categorical_cols= cat_cols, \n                 target_col = True)\n\ndata_train.read_data() # read data path \ndata_train.data_prep() # data preprocessing \n\ntrain_df = data_train.get_data()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:32.085564Z","iopub.execute_input":"2024-09-05T12:55:32.086313Z","iopub.status.idle":"2024-09-05T12:55:48.866740Z","shell.execute_reply.started":"2024-09-05T12:55:32.086259Z","shell.execute_reply":"2024-09-05T12:55:48.865496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test data \ndata_test = Data(path = test_path,\n                 common_columns = common_cols, \n                 categorical_cols= cat_cols, \n                 target_col = False)\ndata_test.read_data()\ndata_test.data_prep()\n\ntest_df = data_test.get_data()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:48.868143Z","iopub.execute_input":"2024-09-05T12:55:48.868515Z","iopub.status.idle":"2024-09-05T12:55:48.915563Z","shell.execute_reply.started":"2024-09-05T12:55:48.868474Z","shell.execute_reply":"2024-09-05T12:55:48.913990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create sub dataset unseen to the model in order u update models weights","metadata":{}},{"cell_type":"code","source":"train_df = train_df[intersection_cols]\ntest_df = test_df[intersection_cols[:-1]] # remove target\nprint(f'Train/Val Data:{train_df.shape}\\nTest Data:{test_df.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:48.917462Z","iopub.execute_input":"2024-09-05T12:55:48.918017Z","iopub.status.idle":"2024-09-05T12:55:48.943492Z","shell.execute_reply.started":"2024-09-05T12:55:48.917972Z","shell.execute_reply":"2024-09-05T12:55:48.941693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs_splits = DataFrame_Split(train_df)\n\ndfs_splits.main()\nsub_dfs = dfs_splits.get_sub_df()\n\ntrain_df_ratio_smote, update_train_df = sub_dfs[0], sub_dfs[1]","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:48.945475Z","iopub.execute_input":"2024-09-05T12:55:48.945982Z","iopub.status.idle":"2024-09-05T12:55:49.234923Z","shell.execute_reply.started":"2024-09-05T12:55:48.945939Z","shell.execute_reply":"2024-09-05T12:55:49.233412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = Ratio_data(df = train_df_ratio_smote, \n                         ratio = 0.1)\n\nsplit.data_ratio()\nsplit.split_data(test_size = 0.2)\nsplit.train_smote()\n\n#X_train, y_train, X_val, y_val = split.get_imbalance_data()\nX_train_res, y_train_res, X_val, y_val = split.get_balance_data()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:49.236631Z","iopub.execute_input":"2024-09-05T12:55:49.237072Z","iopub.status.idle":"2024-09-05T12:55:49.397437Z","shell.execute_reply.started":"2024-09-05T12:55:49.237030Z","shell.execute_reply":"2024-09-05T12:55:49.396178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn = Neural_Network(X_train = X_train_res, \n                    y_train = y_train_res, \n                    X_val = X_val, \n                    y_val = y_val) \nnn.model_compile()\nnn.model_summary()\nnn.model_fit()\nnn.model_history()\nnn.model_roc_curve()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:55:49.399076Z","iopub.execute_input":"2024-09-05T12:55:49.399551Z","iopub.status.idle":"2024-09-05T12:56:13.681204Z","shell.execute_reply.started":"2024-09-05T12:55:49.399499Z","shell.execute_reply":"2024-09-05T12:56:13.679701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trained_model = nn.get_model()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:56:13.683176Z","iopub.execute_input":"2024-09-05T12:56:13.683659Z","iopub.status.idle":"2024-09-05T12:56:13.690258Z","shell.execute_reply.started":"2024-09-05T12:56:13.683612Z","shell.execute_reply":"2024-09-05T12:56:13.688587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unseen Data ","metadata":{}},{"cell_type":"code","source":"unseen_id = [ \"ISIC_0015657\", \"ISIC_0015729\", \"ISIC_0015740\"]","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:56:13.691922Z","iopub.execute_input":"2024-09-05T12:56:13.692347Z","iopub.status.idle":"2024-09-05T12:56:13.705043Z","shell.execute_reply.started":"2024-09-05T12:56:13.692305Z","shell.execute_reply":"2024-09-05T12:56:13.703378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = trained_model.predict(test_df)\npredictions","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:56:13.706924Z","iopub.execute_input":"2024-09-05T12:56:13.707373Z","iopub.status.idle":"2024-09-05T12:56:13.804920Z","shell.execute_reply.started":"2024-09-05T12:56:13.707333Z","shell.execute_reply":"2024-09-05T12:56:13.803163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = [p[0].astype(np.float64) for p in predictions ]\npred","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:56:13.806724Z","iopub.execute_input":"2024-09-05T12:56:13.807156Z","iopub.status.idle":"2024-09-05T12:56:13.817234Z","shell.execute_reply.started":"2024-09-05T12:56:13.807114Z","shell.execute_reply":"2024-09-05T12:56:13.815488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.DataFrame({\"isic_id\":unseen_id, \"target\":pred})","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:56:13.819277Z","iopub.execute_input":"2024-09-05T12:56:13.819827Z","iopub.status.idle":"2024-09-05T12:56:13.830564Z","shell.execute_reply.started":"2024-09-05T12:56:13.819780Z","shell.execute_reply":"2024-09-05T12:56:13.829199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:56:13.832097Z","iopub.execute_input":"2024-09-05T12:56:13.832528Z","iopub.status.idle":"2024-09-05T12:56:13.859226Z","shell.execute_reply.started":"2024-09-05T12:56:13.832486Z","shell.execute_reply":"2024-09-05T12:56:13.857153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2024-09-05T12:56:13.861257Z","iopub.execute_input":"2024-09-05T12:56:13.861853Z","iopub.status.idle":"2024-09-05T12:56:13.872953Z","shell.execute_reply.started":"2024-09-05T12:56:13.861802Z","shell.execute_reply":"2024-09-05T12:56:13.871260Z"},"trusted":true},"execution_count":null,"outputs":[]}]}